>   **SENG 637 - Dependability and Reliability of Software Systems**

**Lab. Report \#1 – Introduction to Testing and Defect Tracking**

| Group: 22      |
|-----------------|
| Student 1 Jeremy Sugimoto                |   
| Student 2 Matt De Filippo             |   
| Student 3 Momin Muhammad               |   
| Student 4 Redge Santillan                |   
| Student 5 Shad Sajid               |   


**Table of Contents**

(When you finish writing, update the following list using right click, then
“Update Field”)

[1 Introduction	1](#_Toc439194677)

[2 High-level description of the exploratory testing plan	1](#_Toc439194678)

[3 Comparison of exploratory and manual functional testing	1](#_Toc439194679)

[4 Notes and discussion of the peer reviews of defect reports	1](#_Toc439194680)

[5 How the pair testing was managed and team work/effort was
divided	1](#_Toc439194681)

[6 Difficulties encountered, challenges overcome, and lessons
learned	1](#_Toc439194682)

# Introduction

In our group assignment, we conducted exploratory testing, manual scripted testing, and regression testing on two versions of ATM simulation software: `ATM System - Lab 1 Version 1.0.jar` and its subsequent release, `ATM System - Lab 1 Version 1.1.jar`. Throughout this process, we documented and tracked the bugs identified using a Jira project.

This document will outline how we performed exploratory and manual tests, highlighting the difference between the two approaches. Additionally, we will be discussing how we collaborated and divided the work amongst ourselves, and any challenges we encountered along the way.

# High-level description of the exploratory testing plan

At a high level, the approach our group took for the exploratory testing plan was to break off into two groups and split the functionality of the app to explore between them. Each group was then responsible for sifting through the general functionality of their portion of the SUT and reporting any bugs they found while using the SUT in a normal manner. To go into more detail, the first group was responsible for exploring login functionality as well as withdrawal and deposit functions. The second group was responsible for exploring the transfer and inquiry functions. This way, the team ensured that every aspect of the app had been interacted with in some form and no feature had been missed.

# Comparison of exploratory and manual functional testing - Matt

The exploratory testing was performed in accordance with the described testing plan. The team split off into two groups to test the entire range of SUT functionality. Once this was completed, the team performed manual functional testing as per the provided use cases (see Appendix C).

The majority of bugs were initially found during the exploratory testing phase and then again found during the manual functional testing. There were, however, some bugs that were found exclusively in the exploratory testing phase. For example, a severe bug was found during exploratory testing. When the user was prompted to select either “1” to continue or “2” to end the session, pressing any other number on the keypad would result in the ATM ejecting $20. This bug could only be found during exploratory testing as there was no test case provided for this scenario. There were also some bugs that were found exclusively during the manual function testing. For example, a bug was while executing test case 34 in which the generated ATM receipt did not display the correct card number. This was an elusive bug that was overlooked during the exploratory testing.

Based on our results, we can infer that each type of testing is useful. The exploratory testing allowed us to test the software in an unrestricted way. It therefore allowed us to find bugs that were more obscure and not captured by the predefined test cases. It was difficult to know when “enough” bugging was performed but setting a time limit allowed us to mitigate this issue. Manual functional testing provided a more structured method of testing; it allowed us to find bugs that were overlooked during the exploratory test phase. It was more methodical, and therefore felt like a more efficient use of time. However, our results emphasized the importance of a detailed test plan. If we were to simply follow the provided test cases and not perform any exploratory testing, severe bugs would have been missed. 


-   Note that you need to submit a report generated by your defect tracking
    system, containing all defects recorded in the system.

# Notes and discussion of the peer reviews of defect reports 

Peer reviews of defect reports for each stage of testing (exploratory, manual scripted and regression) ensured completeness and accurracy of the report. The following required aspects of the defect reports were reviewed for completeness and clarity:
-   The function being tested
-   The initial state of the system
-   Detailed steps to reproduce the defect/bug
-   The expected outcome 
-   The actual outcome
-   The priority or severity of the bug
-   The version of SUT in which the bug was found.
The defects were also resimulated by the peer to validate their existence.

The peer reviews were very helpful as they caught typos, lack of clarity, and defects that didn't actually exist.


# How the pair testing was managed and team work/effort was divided 

Teamwork for testing was divided based on SUT use cases (refer to Appendix C). Shad Sajid, Matt De Filippo, and Momin Muhammad were responsible for exploratory (manual non-scripted) testing, manual scripted testing, and regression testing (re-testing a system after it has been changed) of system startup, session, transaction, withdrawal, and deposit functionalities. Redge Santillan and Jeremy Sugimoto were responsible for exploratory (manual non-scripted) testing, manual scripted testing, and regression testing of transfer, inquiry, and invalid PIN extension functionalities.

For each pair (Redge Santillan and Jeremy Sugimoto as one pair, Shad Sajid, Matt De Filippo, and Momin Muhammad as the other), pair testing was managed using Discord to meet and switching the driver (who manages the keyboard and mouse) and analyst(s) (who analyze the results) periodically.

Teamwork for report completion was divided by section. Redge Santillan completed the introduction, Jeremy Sugimoto completed the high-level description of the exploratory testing plan, Matt De Filippo completed the comparison of exploratory and manual functional testing, Shad Sajid completed the notes and discussion of the peer reviews of defect reports, and Momin Muhammad completed how the pair testing was managed and how teamwork/effort was divided. The Reflection and Learning section was completed by each member and combined.

# Difficulties encountered, challenges overcome, and lessons learned - 

Several obstacles were encountered throughout this assignment. Jira was a new platform to all team members, therefore it took some time to properly set up the project and create a template for reporting bugs. One of the obstacles concerning Jira was that Jira projects have lots of different ways to configure. The team reviewed the provided Jira documentation and resources to become more familiar with this reporting software. This resulted in an efficient workflow that made sense to everyone on the team.

An issue was also encountered with the provided SUT use cases that were used to perform the manual scripted testing. Based on our bug finding, it was determined that the provided use cases were not detailed enough to fully test the software. There were several bugs that could have been missed if exploratory testing was not performed initially. This emphasizes the importance of having a comprehensive list of use cases when performing manual scripted testing. The use cases should also cover some non-anticipated user inputs to ensure that the system can not be broken in unexpected ways.

